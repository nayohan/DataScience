{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정형화 Weight Regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-06f1bd5ba32a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 정형화는 weight_decay로 줄 수 있습니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# 정형화는 weight_decay로 줄 수 있습니다.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 드롭아웃 Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 드롭아웃을 중간중간에 넣어줌으로써 모델이 오버피팅하는 경우 이를 어느정도 극복할 수 있습니다.\n",
    "# 정형화에서 눈치채신분도 계시겠지만 오버피팅하지 않는 상태에서 정형화나 드롭아웃을 넣으면 오히려 학습이 잘 안됩니다.\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(1,16,3,padding=1),  # 28\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout2d(0.2),\n",
    "            nn.Conv2d(16,32,3,padding=1), # 28\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout2d(0.2),\n",
    "            nn.MaxPool2d(2,2),            # 14\n",
    "            nn.Conv2d(32,64,3,padding=1), # 14\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout2d(0.2),\n",
    "            nn.MaxPool2d(2,2)             # 7\n",
    "        )\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(64*7*7,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(100,10)\n",
    "        )\n",
    "        \n",
    "        #초기화 위치\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.layer(x)\n",
    "        out = out.view(batch_size,-1)\n",
    "        out = self.fc_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 초기화 Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      # 초기화 하는 방법\n",
    "        # 모델의 모듈을 차례대로 불러옵니다.\n",
    "        for m in self.modules():\n",
    "            # 만약 그 모듈이 nn.Conv2d인 경우\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                '''\n",
    "                # 작은 숫자로 초기화하는 방법\n",
    "                # 가중치를 평균 0, 편차 0.02로 초기화합니다.\n",
    "                # 편차를 0으로 초기화합니다.\n",
    "                m.weight.data.normal_(0.0, 0.02)\n",
    "                m.bias.data.fill_(0)\n",
    "                \n",
    "                # Xavier Initialization\n",
    "                # 모듈의 가중치를 xavier normal로 초기화합니다.\n",
    "                # 편차를 0으로 초기화합니다.\n",
    "                init.xavier_normal(m.weight.data)\n",
    "                m.bias.data.fill_(0)\n",
    "                '''\n",
    "                \n",
    "                # Kaming Initialization\n",
    "                # 모듈의 가중치를 kaming he normal로 초기화합니다.\n",
    "                # 편차를 0으로 초기화합니다.\n",
    "                init.kaiming_normal_(m.weight.data)\n",
    "                m.bias.data.fill_(0)\n",
    "            \n",
    "            # 만약 그 모듈이 nn.Linear인 경우\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                '''\n",
    "                # 작은 숫자로 초기화하는 방법\n",
    "                # 가중치를 평균 0, 편차 0.02로 초기화합니다.\n",
    "                # 편차를 0으로 초기화합니다.\n",
    "                m.weight.data.normal_(0.0, 0.02)\n",
    "                m.bias.data.fill_(0)\n",
    "                \n",
    "                # Xavier Initialization\n",
    "                # 모듈의 가중치를 xavier normal로 초기화합니다.\n",
    "                # 편차를 0으로 초기화합니다.\n",
    "                init.xavier_normal(m.weight.data)\n",
    "                m.bias.data.fill_(0)\n",
    "                '''\n",
    "                \n",
    "                # Kaming Initialization\n",
    "                # 모듈의 가중치를 kaming he normal로 초기화합니다.\n",
    "                # 편차를 0으로 초기화합니다.\n",
    "                init.kaiming_normal_(m.weight.data)\n",
    "                m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습률(Learning_Rate_Decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = CNN().to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 지정한 스텝 단위로 학습률에 감마를 곱해 학습률을 감소시킵니다\n",
    "#scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma= 0.99)       \n",
    "\n",
    "# 지정한 스텝 지점(예시에서는 10,30,80)마다 학습률에 감마를 곱해줍니다.\n",
    "#scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[10,30,80], gamma= 0.1)  \n",
    "\n",
    "# 매 epoch마다 학습률에 감마를 곱해줍니다.\n",
    "#scheduler = lr_scheduler.ExponentialLR(optimizer, gamma= 0.99)                             \n",
    "\n",
    "# https://pytorch.org/docs/stable/optim.html?highlight=lr_scheduler#torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "# 지정한 메트릭으로 측정한 값이 더 나아지지 않으면 학습률을 감소시킵니다. ex) 정확도, dice score 등등\n",
    "# 이 스케쥴러에는 다양한 인자가 들어가는데 각각의 역할은 도큐먼트를 참고 바랍니다.\n",
    "# 여기서는 patience 즉, 지정한 값이 줄어들지 않을때 몇 epoch 만큼을 지켜볼 것인지를 1로 낮춰놨기 때문에 매 epoch 마다 학습률이 감소하는것을 확인할 수 있습니다.\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,threshold=1,patience=1,mode='min')    \n",
    "\n",
    "# 참고 https://www.geeksforgeeks.org/python-dir-function/\n",
    "print(dir(scheduler))\n",
    "print(dir(optimizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 입력 데이터 정규화(Data_Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화는 transform을 통해 가능합니다.\n",
    "# 여기서 mean, std는 미리 계산된 값입니다.\n",
    "# 각각이 1개인 이유는 MNIST 데이터의 채널이 하나이기 때문입니다. \n",
    "\n",
    "mnist_train = dset.MNIST(\"./\", train=True, \n",
    "                         transform=transforms.Compose([\n",
    "                             transforms.ToTensor(),\n",
    "                             transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "                         ]),\n",
    "                         target_transform=None, \n",
    "                         download=True)\n",
    "mnist_test = dset.MNIST(\"./\", train=False, \n",
    "                        transform=transforms.Compose([\n",
    "                             transforms.ToTensor(),\n",
    "                             transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "                        ]),\n",
    "                        target_transform=None, \n",
    "                        download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 배치 정규화(Batch_Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 데이터를 정규화하는것처럼 연산을 통과한 결과값을 정규화할 수 있습니다.\n",
    "# 그 다양한 방법중에 대표적인것이 바로 Batch Normalization이고 이는 컨볼루션 연산처럼 모델에 한 층으로 구현할 수 있습니다.\n",
    "# https://pytorch.org/docs/stable/nn.html?highlight=batchnorm#torch.nn.BatchNorm2d\n",
    "# nn.BatchNorm2d(x)에서 x는 입력으로 들어오는 채널의 개수입니다.\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(1,16,3,padding=1),  # 28 x 28\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16,32,3,padding=1), # 28 x 28\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),            # 14 x 14\n",
    "            nn.Conv2d(32,64,3,padding=1), # 14 x 14\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2)             #  7 x 7\n",
    "        )\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(64*7*7,100),\n",
    "            nn.BatchNorm1d(100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,10)\n",
    "        )       \n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.layer(x)\n",
    "        out = out.view(batch_size,-1)\n",
    "        out = self.fc_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치정규화나 드롭아웃은 학습할때와 테스트 할때 다르게 동작하기 때문에 모델을 evaluation 모드로 바꿔서 테스트해야합니다.\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  for image,label in test_loader:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 경사하강법의 변형(Gradient_Descent_Variants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 경사하강법 알고리즘이 파이토치에 이미 구현이 되어 있기 때문에 단순하게 이름만 바꿈으로써 해당 알고리즘을 쓸 수 있습니다.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
